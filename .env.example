# LLM Provider Configuration
LLM_PROVIDER=openai  # openai, anthropic, bedrock, groq
LLM_MODEL=gpt-4  # gpt-4, claude-3-opus, anthropic.claude-3-opus-20240229-v1:0, llama3-70b-8192
LLM_API_KEY=your_api_key_here

# Cache Configuration
USE_CACHE=true
CACHE_TYPE=memory  # memory, dynamodb
CACHE_TTL=86400  # 24 hours in seconds

# AWS Configuration (for DynamoDB cache and CloudWatch logging)
DYNAMODB_TABLE=intelligent-llm-agent-cache-dev
AWS_REGION=us-east-1

# Logging Configuration
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# CloudWatch Configuration
CLOUDWATCH_NAMESPACE=IntelligentLLMAgent
CLOUDWATCH_LOG_GROUP=/aws/lambda/intelligent-llm-agent-dev
